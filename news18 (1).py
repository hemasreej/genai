# -*- coding: utf-8 -*-
"""news18

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10OERNN8dIah9jVj3O2ZhDbSv3rdQDGtK
"""

pip install Flask opencv-python opencv-python-headless numpy transformers torch

pip install gradio

pip install gradio opencv-python

!pip install opencv-python numpy transformers

!pip install opencv-python numpy moviepy speechrecognition transformers

!pip install opencv-python numpy transformers moviepy git+https://github.com/openai/whisper.git

import cv2
import numpy as np
import whisper
from transformers import pipeline
import ipywidgets as widgets
from IPython.display import display, clear_output
import moviepy.editor as mp
import json
import os

# Step 1: Initialize the models
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
whisper_model = whisper.load_model("base")

# Step 2: Function to extract audio from video
def extract_audio(video_path):
    audio_path = "audio.wav"
    try:
        video = mp.VideoFileClip(video_path)
        if video.audio is not None:
            video.audio.write_audiofile(audio_path)
        else:
            raise ValueError("No audio stream found in the video.")
    except Exception as e:
        print(f"Error extracting audio: {e}")
        return None
    return audio_path

# Step 3: Function for tampering detection using Optical Flow and Frame Differencing
def detect_tampering(video_path):
    cap = cv2.VideoCapture(video_path)
    tampered = False

    # Initialize variables for optical flow
    ret, previous_frame = cap.read()
    if not ret:
        print("Failed to read the video.")
        return tampered, []

    previous_gray = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)

    frame_count = 0
    tampered_frames = []

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        current_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        # Frame differencing
        frame_diff = cv2.absdiff(previous_gray, current_gray)
        non_zero_count = np.count_nonzero(frame_diff)

        # Check if tampering is detected based on motion and frame difference
        if non_zero_count > 10000:  # Threshold for detecting tampering
            tampered = True
            tampered_frames.append(frame_count)

        previous_gray = current_gray
        frame_count += 1

    cap.release()
    return tampered, tampered_frames

# Step 4: Function to detect CGI elements based on general characteristics
def detect_cgi_general(frame, previous_frame):
    # Convert frames to grayscale for analysis
    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    gray_previous_frame = cv2.cvtColor(previous_frame, cv2.COLOR_BGR2GRAY)

    # Compute the absolute difference between frames
    frame_diff = cv2.absdiff(gray_frame, gray_previous_frame)
    non_zero_count = np.count_nonzero(frame_diff)

    # Analyze color histogram
    hist_current = cv2.calcHist([frame], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])
    hist_previous = cv2.calcHist([previous_frame], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])
    cv2.normalize(hist_current, hist_current, alpha=0, beta=1, norm_type=cv2.NORM_L2)
    cv2.normalize(hist_previous, hist_previous, alpha=0, beta=1, norm_type=cv2.NORM_L2)
    hist_diff = cv2.compareHist(hist_current, hist_previous, cv2.HISTCMP_BHATTACHARYYA)

    # Check if the differences in motion or color are significant
    motion_threshold = 10000  # Threshold for motion detection
    color_threshold = 0.1      # Threshold for color histogram difference

    # Identify if there are significant differences
    if non_zero_count > motion_threshold or hist_diff > color_threshold:
        return True  # Possible CGI detected
    return False  # No CGI detected

# Step 5: Function for audio transcription and summarization
def generate_summary(video_path):
    audio_path = extract_audio(video_path)
    if audio_path is None:
        return "Audio extraction failed.", "Audio extraction failed."

    transcription = whisper_model.transcribe(audio_path)['text']

    # Summarize the transcription
    summary = summarizer(transcription, max_length=100, min_length=30, do_sample=False)
    return summary[0]['summary_text'], transcription

# Step 6: Function to extract metadata from video
def extract_metadata(video_path):
    metadata = {}
    video = mp.VideoFileClip(video_path)
    metadata['duration'] = video.duration  # Duration in seconds
    metadata['fps'] = video.fps  # Frames per second
    metadata['size'] = video.size  # Resolution
    return metadata

# Step 7: Create an upload button
uploader = widgets.FileUpload(accept='.mp4', multiple=False)

# Step 8: Define a function to handle video processing
def on_upload_change(change):
    if uploader.value:
        # Clear previous outputs
        clear_output(wait=True)
        display(uploader)

        video_file = list(uploader.value.keys())[0]
        content = uploader.value[video_file]['content']

        with open(video_file, 'wb') as f:
            f.write(content)

        # Process the video
        print("Processing video...")

        # Extract metadata
        metadata = extract_metadata(video_file)
        print(f"Metadata: {metadata}")

        # Detect tampering
        tampered, tampered_frames = detect_tampering(video_file)

        # Check for CGI
        cgi_detected_frames = []
        cap = cv2.VideoCapture(video_file)
        frame_count = 0
        ret, previous_frame = cap.read()

        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break

            if detect_cgi_general(frame, previous_frame):
                cgi_detected_frames.append(frame_count)

            previous_frame = frame
            frame_count += 1

        cap.release()

        # Generate summary
        summary, transcription = generate_summary(video_file)

        # Log results
        results = {
            "tampered": tampered,
            "tampered_frames": tampered_frames,
            "cgi_detected_frames": cgi_detected_frames,
            "summary": summary,
            "transcription": transcription,
            "metadata": metadata
        }

        # Save results to JSON
        with open("results.json", "w") as json_file:
            json.dump(results, json_file, indent=4)

        # Display results
        print(f"Video Tampering Detected: {tampered}")
        print(f"Tampered Frames: {tampered_frames}")
        print(f"CGI Detected Frames: {cgi_detected_frames}")
        print(f"Summary: {summary}")

# Step 9: Link the uploader to the processing function
uploader.observe(on_upload_change, names='value')

display(uploader)

